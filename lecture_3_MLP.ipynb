{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osamja/zero-to-hero/blob/main/lecture_3/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "5onyiSmq5Kqz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Cheatsheet: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt    # Uncomment to download names.txt\n",
        "names = open('names.txt', 'r').read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dictionary of characters and their indices\n",
        "chars = sorted(list(set(''.join(names))))\n",
        "char2idx = {c: i+1 for i, c in enumerate(chars)}\n",
        "idx2char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "char2idx['.'] = 0  # ? andrej has this set to zero but doesn't that replace the first character in the alphabet?\n",
        "idx2char[0] = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of examples: 5\n"
          ]
        }
      ],
      "source": [
        "# Build the dataset\n",
        "block_size = 3 # context length: how many characters to consider before predicting the next character\n",
        "X, Y = [], []\n",
        "\n",
        "for name in names[:1]:\n",
        "    name = name + '.'\n",
        "    context = [0] * block_size      # The first context is a block of zeros which enables padding\n",
        "    for c in name:\n",
        "        X.append(context)\n",
        "        Y.append(char2idx[c])      # The next character is context[-1] (the last character in the context)\n",
        "        # print(''.join([idx2char[i] for i in context]), '->', c)\n",
        "        context = context[1:] + [char2idx[c]]   # Crop & shift the context by one character\n",
        "        \n",
        "X = torch.tensor(X, dtype=torch.long)   # Should we be using long or ints?\n",
        "Y = torch.tensor(Y, dtype=torch.long)\n",
        "\n",
        "num_examples = X.shape[0]\n",
        "print('Number of examples:', num_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True])"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Embed the characters\n",
        "x_enc = torch.nn.functional.one_hot(X, num_classes=27).float()\n",
        "x_enc.shape\n",
        "\n",
        "C = torch.randn((27, 2))    # Two dimensional embedding\n",
        "C[5] == F.one_hot(torch.tensor(5), num_classes=27).float() @ C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 3, 2])"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# wow this array indexing is so cool\n",
        "emb = C[X]\n",
        "emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-1.2201,  1.4294])"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb[2][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "# construct the hidden layer\n",
        "W1 = torch.randn(6, 100)\n",
        "b1 = torch.randn(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.0347, -0.7114, -1.0347, -0.7114, -1.0347, -0.7114],\n",
              "        [-1.0347, -0.7114, -1.0347, -0.7114, -0.1496,  0.6886],\n",
              "        [-1.0347, -0.7114, -0.1496,  0.6886, -1.2201,  1.4294],\n",
              "        [-0.1496,  0.6886, -1.2201,  1.4294, -1.2201,  1.4294],\n",
              "        [-1.2201,  1.4294, -1.2201,  1.4294, -1.0414,  1.0597]])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "res = emb.reshape(5, 6)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.0347, -0.7114, -1.0347, -0.7114, -1.0347, -0.7114],\n",
              "        [-1.0347, -0.7114, -1.0347, -0.7114, -0.1496,  0.6886],\n",
              "        [-1.0347, -0.7114, -0.1496,  0.6886, -1.2201,  1.4294],\n",
              "        [-0.1496,  0.6886, -1.2201,  1.4294, -1.2201,  1.4294],\n",
              "        [-1.2201,  1.4294, -1.2201,  1.4294, -1.0414,  1.0597]])"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb.view(5, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[True, True, True, True, True, True],\n",
              "        [True, True, True, True, True, True],\n",
              "        [True, True, True, True, True, True],\n",
              "        [True, True, True, True, True, True],\n",
              "        [True, True, True, True, True, True]])"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -1 means \"whatever is needed to make the shape work\"- copilot. haha same as num_examples\n",
        "emb.view(-1, 6) == emb.reshape(num_examples, 6) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-1.0000), tensor(1.))"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate the hidden layer\n",
        "h = emb.view(5, 6) @ W1 + b1\n",
        "h = torch.tanh(h)\n",
        "h.min(), h.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the output layer\n",
        "W2 = torch.randn(100, 27)\n",
        "b2 = torch.randn(27) \n",
        "\n",
        "logits = h @ W2 + b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 27])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 27])"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's get our fake counts and then normalize them into a probability distribution\n",
        "counts = logits.exp()\n",
        "prob = counts / counts.sum(1, keepdims=True)\n",
        "prob.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([8.5821e-13, 1.9780e-10, 5.6864e-15, 3.1944e-02, 2.0523e-03])"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Lets use y to get the actual probability\n",
        "prob[torch.arange(num_examples), Y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOkY45OAMkpFzYuL1GYgdPQ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bebb980880604d236ddcb4913ad10a36ac6cae9759cdda587ed688a750b11c5c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
